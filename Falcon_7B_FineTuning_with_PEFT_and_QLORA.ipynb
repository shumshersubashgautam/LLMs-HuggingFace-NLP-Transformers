{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Falcon finetuning on openassistant-guanaco**"
      ],
      "metadata": {
        "id": "uauairpe69Ww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install datasets\n",
        "# !pip install peft"
      ],
      "metadata": {
        "id": "UBhoHuiz7eHD"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install trl"
      ],
      "metadata": {
        "id": "5VlFWOjN8E4A"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6r4pzawz6pnh",
        "outputId": "072ed65c-57b0-4b8a-9223-41edde2c3e34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    HfArgumentParser,\n",
        "    TrainingArguments,\n",
        ")\n",
        "from peft.tuners.lora import LoraLayer\n",
        "from trl import SFTTrainer\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Optional\n",
        "\n",
        "@dataclass\n",
        "class ModelArguments:\n",
        "    \"\"\"\n",
        "    Arguments for creating and preparing the model.\n",
        "    \"\"\"\n",
        "    model_name: str = field(\n",
        "        default=\"tiiuae/falcon-7b\",\n",
        "        metadata={\"help\": \"The model name or path from the Hugging Face hub.\"},\n",
        "    )\n",
        "    use_4bit: bool = field(\n",
        "        default=True,\n",
        "        metadata={\"help\": \"Activate 4bit precision base model loading\"},\n",
        "    )\n",
        "    use_nested_quant: bool = field(\n",
        "        default=False,\n",
        "        metadata={\"help\": \"Activate nested quantization for 4bit base models\"},\n",
        "    )\n",
        "    bnb_4bit_compute_dtype: str = field(\n",
        "        default=\"float16\",\n",
        "        metadata={\"help\": \"Compute dtype for 4bit base models\"},\n",
        "    )\n",
        "    bnb_4bit_quant_type: str = field(\n",
        "        default=\"nf4\",\n",
        "        metadata={\"help\": \"Quantization type: fp4 or nf4\"},\n",
        "    )\n",
        "    lora_alpha: int = field(default=16)\n",
        "    lora_dropout: float = field(default=0.1)\n",
        "    lora_r: int = field(default=64)\n",
        "\n",
        "@dataclass\n",
        "class ScriptArguments:\n",
        "    \"\"\"\n",
        "    Arguments for model training and data handling.\n",
        "    \"\"\"\n",
        "    local_rank: int = field(default=-1, metadata={\"help\": \"Used for multi-gpu\"})\n",
        "    per_device_train_batch_size: int = field(default=4)\n",
        "    per_device_eval_batch_size: Optional[int] = field(default=1)\n",
        "    gradient_accumulation_steps: Optional[int] = field(default=4)\n",
        "    learning_rate: Optional[float] = field(default=2e-4)\n",
        "    max_grad_norm: Optional[float] = field(default=0.3)\n",
        "    weight_decay: Optional[int] = field(default=0.001)\n",
        "    max_seq_length: Optional[int] = field(default=512)\n",
        "    dataset_name: Optional[str] = field(\n",
        "        default=\"timdettmers/openassistant-guanaco\",\n",
        "        metadata={\"help\": \"The preference dataset to use.\"},\n",
        "    )\n",
        "    num_train_epochs: Optional[int] = field(\n",
        "        default=1,\n",
        "        metadata={\"help\": \"The number of training epochs for the reward model.\"},\n",
        "    )\n",
        "    fp16: Optional[bool] = field(\n",
        "        default=False,\n",
        "        metadata={\"help\": \"Enables fp16 training.\"},\n",
        "    )\n",
        "    bf16: Optional[bool] = field(\n",
        "        default=False,\n",
        "        metadata={\"help\": \"Enables bf16 training.\"},\n",
        "    )\n",
        "    packing: Optional[bool] = field(\n",
        "        default=False,\n",
        "        metadata={\"help\": \"Use packing dataset creating.\"},\n",
        "    )\n",
        "    gradient_checkpointing: Optional[bool] = field(\n",
        "        default=True,\n",
        "        metadata={\"help\": \"Enables gradient checkpointing.\"},\n",
        "    )\n",
        "    optim: Optional[str] = field(\n",
        "        default=\"paged_adamw_32bit\",\n",
        "        metadata={\"help\": \"The optimizer to use.\"},\n",
        "    )\n",
        "    lr_scheduler_type: str = field(\n",
        "        default=\"constant\",\n",
        "        metadata={\"help\": \"Learning rate schedule. Constant a bit better than cosine, and has advantage for analysis\"},\n",
        "    )\n",
        "    max_steps: int = field(default=10000, metadata={\"help\": \"How many optimizer update steps to take\"})\n",
        "    warmup_ratio: float = field(default=0.03, metadata={\"help\": \"Fraction of steps to do a warmup for\"})\n",
        "    group_by_length: bool = field(\n",
        "        default=True,\n",
        "        metadata={\n",
        "            \"help\": \"Group sequences into batches with same length. Saves memory and speeds up training considerably.\"\n",
        "        },\n",
        "    )\n",
        "    save_steps: int = field(default=10, metadata={\"help\": \"Save checkpoint every X updates steps.\"})\n",
        "    logging_steps: int = field(default=10, metadata={\"help\": \"Log every X updates steps.\"})\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model_peftconfig_tokenizer(args: ModelArguments):\n",
        "    \"\"\"\n",
        "    Create the model, tokenizer, and peft_config based on provided arguments.\n",
        "    \"\"\"\n",
        "    compute_dtype = getattr(torch, args.bnb_4bit_compute_dtype)\n",
        "\n",
        "    # Configure BitsAndBytes for model quantization\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=args.use_4bit,\n",
        "        bnb_4bit_quant_type=args.bnb_4bit_quant_type,\n",
        "        bnb_4bit_compute_dtype=compute_dtype,\n",
        "        bnb_4bit_use_double_quant=args.use_nested_quant,\n",
        "    )\n",
        "\n",
        "    # Alert for bfloat16 acceleration support\n",
        "    if compute_dtype == torch.float16 and args.use_4bit:\n",
        "        major, _ = torch.cuda.get_device_capability()\n",
        "        if major >= 8:\n",
        "            print(\"=\" * 80)\n",
        "            print(\"Your GPU supports bfloat16, you can accelerate training with --bf16\")\n",
        "            print(\"=\" * 80)\n",
        "\n",
        "    # Load the model with quantization configuration\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        args.model_name, quantization_config=bnb_config, device_map={\"\": 0}, trust_remote_code=True\n",
        "    )\n",
        "\n",
        "    # Define Lora Configuration\n",
        "    peft_config = LoraConfig(\n",
        "        lora_alpha=args.lora_alpha,\n",
        "        lora_dropout=args.lora_dropout,\n",
        "        r=args.lora_r,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "        target_modules=[\n",
        "            \"query_key_value\",\n",
        "            \"dense\",\n",
        "            \"dense_h_to_4h\",\n",
        "            \"dense_4h_to_h\",\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    # Load the tokenizer and set padding token\n",
        "    tokenizer = AutoTokenizer.from_pretrained(args.model_name, trust_remote_code=True)\n",
        "\n",
        "    # Need to do below for models like Falcon-7B, GPT-2 etc,\n",
        "    # because it doesn't have an official pad token.\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    return model, peft_config, tokenizer"
      ],
      "metadata": {
        "id": "vnhHXFrs7GvV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_arguments():\n",
        "    \"\"\"\n",
        "    Parse Model and Script Arguments.\n",
        "    Returns:\n",
        "        ModelArguments, ScriptArguments\n",
        "    \"\"\"\n",
        "    parser = HfArgumentParser((ModelArguments, ScriptArguments))\n",
        "    return parser.parse_args_into_dataclasses()\n",
        "\n",
        "def load_training_data(dataset_name: str):\n",
        "    \"\"\"\n",
        "    Load dataset for training.\n",
        "    Args:\n",
        "        dataset_name (str): Name or path of the dataset.\n",
        "    Returns:\n",
        "        Dataset object\n",
        "    \"\"\"\n",
        "    return load_dataset(dataset_name, split=\"train\")\n",
        "\n",
        "def get_training_args(script_args: ScriptArguments):\n",
        "    \"\"\"\n",
        "    Get Training Arguments from ScriptArguments.\n",
        "    Args:\n",
        "        script_args (ScriptArguments): Parsed ScriptArguments.\n",
        "    Returns:\n",
        "        TrainingArguments\n",
        "    \"\"\"\n",
        "    return TrainingArguments(\n",
        "        output_dir=\"./results\",\n",
        "        per_device_train_batch_size = script_args.per_device_train_batch_size,\n",
        "        gradient_accumulation_steps=script_args.gradient_accumulation_steps,\n",
        "        optim=script_args.optim,\n",
        "        save_steps=script_args.save_steps,\n",
        "        logging_steps=script_args.logging_steps,\n",
        "        learning_rate=script_args.learning_rate,\n",
        "        fp16=script_args.fp16,\n",
        "        bf16=script_args.bf16,\n",
        "        max_grad_norm=script_args.max_grad_norm,\n",
        "        max_steps=script_args.max_steps,\n",
        "        warmup_ratio=script_args.warmup_ratio,\n",
        "        group_by_length=script_args.group_by_length,\n",
        "        lr_scheduler_type=script_args.lr_scheduler_type,\n",
        "    )\n",
        "\n",
        "def adjust_model_for_bf16(trainer, bf16: bool):\n",
        "    \"\"\"\n",
        "    Adjust Model Layers for bf16.\n",
        "    Args:\n",
        "        trainer (SFTTrainer): Initialized SFTTrainer object.\n",
        "        bf16 (bool): Flag to indicate usage of bf16.\n",
        "    \"\"\"\n",
        "    for name, module in trainer.model.named_modules():\n",
        "        if isinstance(module, LoraLayer) and bf16:\n",
        "            module = module.to(torch.bfloat16)\n",
        "        if \"norm\" in name:\n",
        "            module = module.to(torch.float32)\n",
        "        if \"lm_head\" in name or \"embed_tokens\" in name:\n",
        "            if hasattr(module, \"weight\") and bf16 and module.weight.dtype == torch.float32:\n",
        "                module = module.to(torch.bfloat16)\n",
        "\n",
        "# Main Execution:\n",
        "\n",
        "model_args, script_args = parse_arguments()\n",
        "\n",
        "model, peft_config, tokenizer = get_model_peftconfig_tokenizer(model_args)\n",
        "model.config.use_cache = False\n",
        "\n",
        "dataset = load_training_data(script_args.dataset_name)\n",
        "\n",
        "training_arguments = get_training_args(script_args)\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    peft_config=peft_config,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=script_args.max_seq_length,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_arguments,\n",
        "    packing=script_args.packing,\n",
        ")\n",
        "\n",
        "adjust_model_for_bf16(trainer, script_args.bf16)\n",
        "\n",
        "# Train the Model\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "YsyUsvh17JsV",
        "outputId": "a980ae0f-35f4-4050-86eb-c9a263aaa1ab"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-0f25947b281d>\u001b[0m in \u001b[0;36m<cell line: 63>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;31m# Main Execution:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m \u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscript_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_arguments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpeft_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model_peftconfig_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-0f25947b281d>\u001b[0m in \u001b[0;36mparse_arguments\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \"\"\"\n\u001b[1;32m      7\u001b[0m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHfArgumentParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModelArguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mScriptArguments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_args_into_dataclasses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_training_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/hf_argparser.py\u001b[0m in \u001b[0;36mparse_args_into_dataclasses\u001b[0;34m(self, args, return_remaining_strings, look_for_args_file, args_filename, args_file_flag)\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mremaining_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Some specified arguments are not used by the HfArgumentParser: {remaining_args}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Some specified arguments are not used by the HfArgumentParser: ['-f', '/root/.local/share/jupyter/runtime/kernel-e2687684-dc9a-4894-9b58-64f0ffbbd865.json']"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8jstikkJ7uB2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}